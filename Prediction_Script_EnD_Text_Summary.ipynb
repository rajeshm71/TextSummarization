{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prediction_Script_EnD_Text_Summary.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq8KNRtPJ98R",
        "outputId": "0a8bf34c-c73b-4a6a-850c-b0c0e0204b68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_gpu==1.15.0\n",
            "  Downloading tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5 MB 7.6 kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (1.43.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (0.37.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (1.13.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (3.17.3)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 59.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (1.19.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (3.3.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 45.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow_gpu==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15.0) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15.0) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15.0) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15.0) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15.0) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow_gpu==1.15.0) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=ec87aebbb4d8d8efc4c4de1b18b558ccdc265561716af42d9cd85b176c7eb139\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow-gpu\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 1.15.0 which is incompatible.\n",
            "tensorflow 2.7.0 requires tensorflow-estimator<2.8,~=2.7.0rc0, but you have tensorflow-estimator 1.15.1 which is incompatible.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"
          ]
        }
      ],
      "source": [
        "#install tensorflow gpu\n",
        "!pip install tensorflow_gpu==1.15.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'h5py==2.10.0'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve4ODjXrKDId",
        "outputId": "f6f2e49a-ebe4-448d-cad5-8f370b905756"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 24.7 MB/s eta 0:00:01\r\u001b[K     |▎                               | 20 kB 30.1 MB/s eta 0:00:01\r\u001b[K     |▍                               | 30 kB 33.1 MB/s eta 0:00:01\r\u001b[K     |▌                               | 40 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |▋                               | 51 kB 35.7 MB/s eta 0:00:01\r\u001b[K     |▊                               | 61 kB 30.4 MB/s eta 0:00:01\r\u001b[K     |▉                               | 71 kB 26.4 MB/s eta 0:00:01\r\u001b[K     |█                               | 81 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█                               | 92 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 102 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 112 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 122 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 133 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 143 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 153 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 163 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 174 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 184 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 194 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 204 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 215 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 225 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 235 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 245 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 256 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 266 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 276 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 286 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 296 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 307 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 317 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 327 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 337 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 348 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |████                            | 358 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |████                            | 368 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 378 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 389 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 399 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 409 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 419 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 430 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████                           | 440 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████                           | 450 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 460 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 471 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 481 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 491 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 501 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 512 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 522 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 532 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 542 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 552 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 563 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 573 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 583 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 593 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 604 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 614 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 624 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 634 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 645 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 655 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 665 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 675 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 686 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 696 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 706 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 716 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 727 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 737 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 747 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 757 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 768 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 778 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 788 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 798 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 808 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 819 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 829 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 839 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 849 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 860 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 870 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 880 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 890 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 901 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 911 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 921 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 931 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 942 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 952 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 962 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 972 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 983 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 993 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 1.0 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 1.0 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 1.0 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 1.0 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 1.0 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 1.1 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 1.1 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 1.1 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 1.1 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 1.1 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 1.1 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 1.1 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 1.1 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 1.1 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 1.1 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.2 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.2 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 1.2 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 1.2 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 1.2 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 1.2 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 1.2 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 1.2 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 1.2 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.2 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.3 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 1.3 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.3 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.3 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 1.3 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 1.3 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.3 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.3 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.3 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.4 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.4 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 1.4 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.4 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 1.4 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.4 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.4 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.4 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.4 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.4 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.5 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.5 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.5 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.5 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.5 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.5 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.5 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.5 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.5 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.5 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.6 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.6 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.6 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.6 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.6 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.6 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.6 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.6 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.6 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.6 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.7 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.7 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.7 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.7 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.7 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.7 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.7 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.7 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.7 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.8 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.8 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.8 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.8 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.8 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.8 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.8 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.8 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.8 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.8 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.9 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.9 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.9 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.9 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.9 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.9 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.9 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.9 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.9 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.9 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 2.0 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 2.0 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 2.0 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 2.0 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 2.0 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 2.0 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 2.0 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 2.0 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 2.0 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 2.0 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 2.1 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 2.1 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 2.1 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 2.1 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 2.1 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 2.1 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 2.1 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 2.1 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 2.1 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 2.2 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 2.2 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 2.2 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 2.2 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 2.2 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 2.2 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 2.2 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 2.2 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.2 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.2 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 2.3 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 2.3 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 2.3 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 2.3 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 2.3 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 2.3 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 2.3 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.3 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.3 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 2.3 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 2.4 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 2.4 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 2.4 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 2.4 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 2.4 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 2.4 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.4 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.4 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 2.4 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 2.4 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 2.5 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 2.5 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 2.5 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 2.5 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 2.5 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.5 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.5 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 2.5 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 2.5 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 2.5 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 2.6 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 2.6 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 2.6 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 2.6 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.6 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 2.6 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 2.6 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 2.6 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 2.6 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 2.7 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 2.7 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 2.7 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.7 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.7 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 2.7 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.7 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.7 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.7 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 2.7 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.8 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.8 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.8 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.8 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.8 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.8 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 2.8 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.8 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.8 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.8 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.9 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.9 MB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.9 MB 27.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n",
            "Installing collected packages: h5py\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 1.15.0 which is incompatible.\n",
            "tensorflow 2.7.0 requires tensorflow-estimator<2.8,~=2.7.0rc0, but you have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "Successfully installed h5py-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "import numpy as np  \n",
        "import pandas as pd \n",
        "import re \n",
        "import nltk \n",
        "import tensorflow as tf\n",
        "import os\n",
        "import pickle\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K         \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords   \n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "3W7EpxvCKBu0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stopwords from nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "R9ONjVn4SK6A",
        "outputId": "732655a0-974f-4771-f0f0-de3a4674e522",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "klAL-sAgSK9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_PATH = \"/content/drive/MyDrive/news_dataset_csv\"\n",
        "DATA_PATH = os.path.join(ROOT_PATH,\"train-stats.csv\")\n",
        "MODEL_PATH = os.path.join(ROOT_PATH, \"Text_summarization_model.h5\")\n",
        "X_TOKENIZER_PATH = os.path.join(ROOT_PATH, \"x_tokenizer.pickle\")\n",
        "Y_TOKENIZER_PATH = os.path.join(ROOT_PATH, \"y_tokenizer.pickle\")\n",
        "max_len_text=160\n",
        "max_len_summary=30"
      ],
      "metadata": {
        "id": "dl-I8gLoKiF5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(X_TOKENIZER_PATH, 'rb') as handle:\n",
        "    x_tokenizer = pickle.load(handle)\n",
        "\n",
        "with open(Y_TOKENIZER_PATH, 'rb') as handle:\n",
        "    y_tokenizer = pickle.load(handle)"
      ],
      "metadata": {
        "id": "rAtCcH8pRA4c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_voc_size = len(x_tokenizer.word_index) + 1\n",
        "y_voc_size = len(y_tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "FhxRyeHMRIYf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_index_to_word = x_tokenizer.index_word \n",
        "summary_index_to_word = y_tokenizer.index_word \n",
        "summary_word_to_index = y_tokenizer.word_index"
      ],
      "metadata": {
        "id": "q9d9Kq5YW5Ff"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5YgY5VVn-kcs"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state\n",
        "            inputs: (batchsize * 1 * de_in_dim)\n",
        "            states: (batchsize * 1 * de_latent_dim)\n",
        "            \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch size * en_seq_len * latent_dim\n",
        "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>', U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu1t5S-437LL",
        "outputId": "fdd94ae8-9907-437a-d386-56e0eb8a848e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ]
        }
      ],
      "source": [
        "K.clear_session() \n",
        "latent_dim = 500 \n",
        "\n",
        "# Encoder \n",
        "encoder_inputs = Input(shape=(max_len_text,)) \n",
        "enc_emb = Embedding(x_voc_size, latent_dim,trainable=True)(encoder_inputs) \n",
        "\n",
        "#LSTM 1 \n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
        "\n",
        "#LSTM 2 \n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
        "\n",
        "#LSTM 3 \n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n",
        "\n",
        "# Set up the decoder. \n",
        "decoder_inputs = Input(shape=(None,)) \n",
        "dec_emb_layer = Embedding(y_voc_size, latent_dim,trainable=True) \n",
        "dec_emb = dec_emb_layer(decoder_inputs) \n",
        "\n",
        "#LSTM using encoder_states as initial state\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
        "\n",
        "#Attention Layer\n",
        "attn_layer = AttentionLayer(name='attention_layer') \n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
        "\n",
        "# Concat attention output and decoder LSTM output \n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "# Concat attention output and decoder LSTM output \n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "#Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax')) \n",
        "decoder_outputs = decoder_dense(decoder_concat_input) \n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder inference\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# decoder inference\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(max_len_text,latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "#attention inference\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "[decoder_outputs2] + [state_h2, state_c2])"
      ],
      "metadata": {
        "id": "oUm19vEBM2_h"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "\n",
        "    # Chose the 'start' word as the first word of the target sequence\n",
        "    target_seq[0, 0] = summary_word_to_index['start']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, 1:]) + 1 #get index of highest probable predicted word\n",
        "        #print(sampled_token_index)\n",
        "        sampled_token = summary_index_to_word[sampled_token_index] #get word from inde\n",
        "\n",
        "        if(sampled_token != 'end'): #check if word is not 'end'\n",
        "          decoded_sentence += ' ' + sampled_token #concatenate predicted word to the decoded sentence\n",
        "          if len(decoded_sentence.split()) >= (max_len_summary-1):\n",
        "            stop_condition = True\n",
        "            # Exit condition: If predicted word is 'end' or we have predicted maximum words in a sentence then stop\n",
        "        else:\n",
        "          stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "R0OsA3gZNFcE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_texts(text):\n",
        "    text = re.sub(r'\\([^)]*\\)', '', text)    # replace special characters with empty string\n",
        "    text = re.sub('\"','', text)              # replace \" with empty string\n",
        "    text = re.sub(r\"'s\\b\\n\",\"\",text)         # replace 's, \\b, \\n with empty string\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)    # replace words with other than alphabets with space\n",
        "    text = text.lower()                      # transform text into lower case\n",
        "\n",
        "    # split the text into list of words and select only those words which are not stopwords by removing trailing and leading spaces\n",
        "    tokens = [w.strip() for w in text.split() if not w in stop_words] \n",
        "\n",
        "    #select only those words which is atleast 3 character long\n",
        "    long_words = []\n",
        "    for i in tokens:\n",
        "        if len(i) >= 3:                  #removing short word\n",
        "            long_words.append(i)\n",
        "    return (\" \".join(long_words)).strip()"
      ],
      "metadata": {
        "id": "m1nSlrzBNYEF"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"HAMBURG, Germany\\n\\t, June 3  As he left the soccer field after a club match in the eastern German city of Halle on March 25, the Nigerian forward Adebowale Ogungbure was spit upon, jeered with racial remarks and mocked with monkey noises. In rebuke, he placed two fingers under his nose to simulate a Hitler mustache and thrust his arm in a Nazi salute. Marc Zoro, right, an Ivory Coast native, was a target of racial slurs from the home fans in Messina, Italy. Adriano, a star with Inter Milan, tried to persuade him to stay on the field. From now until its conclusion on July 9, Jeff Z. Klein and other staff members of The Times and International Herald Tribune will track the world's most popular sporting event. Your guide to the games in Germany: teams, rosters, schedules, statistics, venues and more. In April, the American defender Oguchi Onyewu, playing for his professional club team in Belgium, dismissively gestured toward fans who were making simian chants at him. Then, as he went to throw the ball inbounds, Onyewu said a fan of the opposing team reached over\\n\\t a barrier and punched him in the face. International soccer has been plagued for years by violence among fans, including racial incidents. But FIFA, soccer's Zurich-based world governing body, said there has been a recent surge in discriminatory behavior toward blacks by fans and other players, an escalation that has dovetailed with the signing of more players from Africa and Latin America by elite European clubs. This \"'deplorable trend,'\" as FIFA has called it, now threatens to embarrass the sport on its grandest stage, the World Cup, which opens June 9 for a monthlong run in 12 cities around Germany. More than 30 billion cumulative television viewers are expected to watch part of the competition and Joseph S. Blatter, FIFA's president, has vowed to crack down on racist behavior during the tournament.\", 'russia russia']"
      ],
      "metadata": {
        "id": "sIpb-j-nOLGk"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predictions(texts):\n",
        "  ''' Input : list of texts\n",
        "      Output : list of summary\n",
        "  '''\n",
        "  stop_words = set(stopwords.words('english'))  #Use set of stopwords from english language\n",
        "  predicted_summary = []\n",
        "  for text in texts:\n",
        "    text = preprocess_texts(text)\n",
        "    text = x_tokenizer.texts_to_sequences([text])\n",
        "    text = pad_sequences(text,  maxlen=max_len_text, padding='post') #It add zeroes to the end of sequence upto max length.\n",
        "    summary = decode_sequence(text.reshape(1,max_len_text))\n",
        "    predicted_summary.append(summary)\n",
        "  return predicted_summary"
      ],
      "metadata": {
        "id": "ka4hW1vBRtTW"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions(texts)"
      ],
      "metadata": {
        "id": "ktNA9Zo0X_FK",
        "outputId": "9bbb77b8-6d2f-4033-c836-9776cc5f2326",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' requests adorns antiblackness clinch clinch medallion cleanliness backflip ghostbustersbuilding thaw detachment surgeons socialflow teenage borer domingo blockers semitism rdeyegirl rdeyegirl lavigne lavigne rugasira ruta sayig winterized philippon raised visalia',\n",
              " ' wayhaught housebuilders zalora sheldon fc librettist lisa ith propped irlan pai bukh bukh bukh bukh shutdowns plishka assets handicapping hewson issuing shorty castparts gryffindor mingardi thrombosis netblazr lacerations irreconcilable']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    }
  ]
}